"""
Summary module for handling large model contexts and reducing redundancy.
Provides a single function to enhance conversation history using model-generated summaries.
"""

from base import function_ai, parameters_func, property_param
import json
import re
from typing import List, Dict, Any, Set, Tuple
import datetime

def ensure_message_dict(msg: Any) -> Dict:
    """
    Ensure message is a dictionary.
    Handles both dict and ChatCompletionMessage/Pydantic objects.
    """
    if isinstance(msg, dict):
        return msg
    # Check if it's a Pydantic model or has model_dump method
    elif hasattr(msg, 'model_dump'):
        return msg.model_dump()
    # Check if it's an object with attributes
    elif hasattr(msg, '__dict__'):
        return vars(msg)
    else:
        # Try to convert to dict
        try:
            return dict(msg)
        except:
            # Fallback: create minimal dict with available attributes
            result = {}
            if hasattr(msg, 'role'):
                result['role'] = msg.role
            if hasattr(msg, 'content'):
                result['content'] = msg.content
            if hasattr(msg, 'tool_calls'):
                result['tool_calls'] = msg.tool_calls
            if hasattr(msg, 'tool_call_id'):
                result['tool_call_id'] = msg.tool_call_id
            if hasattr(msg, 'reasoning_content'):
                result['reasoning_content'] = msg.reasoning_content
            return result

def ensure_messages_dict(messages: List[Any]) -> List[Dict]:
    """Convert list of messages to list of dictionaries."""
    return [ensure_message_dict(msg) for msg in messages]

# ============================================================================
# Summaryå·¥å…·å‚æ•°å®šä¹‰
# ============================================================================

__SUMMARY_PROPERTY_MESSAGES__ = property_param(
    name="messages",
    description="The conversation messages to optimize. System will handle this automatically, you do not need to provide it.",
    t="array"
)

__SUMMARY_PROPERTY_MESSAGES_OPTIMIZED__ = property_param(
    name="messages_optimized",
    description="The optimized conversation messages after applying the summary.",
    t="array"
)

__SUMMARY_PROPERTY_CONTENT__ = property_param(
    name="content",
    description="The model-generated summary content to use for optimization.",
    t="string"
)

# ============================================================================
# Summaryå·¥å…·å‡½æ•°å®šä¹‰
# ============================================================================

__ENHANCE_SUMMARY_FUNCTION__ = function_ai(
    name="enhance_summary",
    description="Use model-generated summary to optimize conversation history, reducing token usage while preserving context.",
    parameters=parameters_func([__SUMMARY_PROPERTY_MESSAGES__, __SUMMARY_PROPERTY_CONTENT__])
)


__ENHANCE_SUMMARY_BY_AI_FUNCTION__ = function_ai(
    name="summary_by_ai",
    description="Just summarize the history messages and update messages",
    parameters=parameters_func([])
)

__ENHANCE_UPDATE_FUNCTION__ = function_ai(
    name="update",
    description="Update the original messages list with the optimized messages.",
    parameters=parameters_func([__SUMMARY_PROPERTY_MESSAGES_OPTIMIZED__])
)

tools = [__ENHANCE_SUMMARY_FUNCTION__]

# ============================================================================
# Summaryæ ¸å¿ƒå®ç° - æ™ºèƒ½ä¿ç•™ç­–ç•¥
# ============================================================================

def is_error_message(message: Dict) -> bool:
    """Check if message contains error information."""
    content = message.get('content', '')
    if not content or not isinstance(content, str):
        return False
    
    error_keywords = ['error', 'Error', 'ERROR', 'fail', 'Fail', 'FAIL', 'exception', 'Exception', 
                      'traceback', 'Traceback', 'failed', 'Failed', 'failure', 'Failure',
                      'é”™è¯¯', 'å¤±è´¥', 'å¼‚å¸¸']
    
    content_lower = content.lower()
    return any(keyword.lower() in content_lower for keyword in error_keywords)

def is_important_tool_result(message: Dict) -> bool:
    """Check if tool result contains important data."""
    if message.get('role') != 'tool':
        return False
    
    content = message.get('content', '')
    if not content or not isinstance(content, str):
        return False
    
    # Important tool results often contain data like file contents, code, etc.
    important_patterns = [
        r'file.*content', r'code.*:', r'data.*:', r'result.*:', r'output.*:',
        r'æˆåŠŸ', r'å®Œæˆ', r'created', r'deleted', r'modified', r'updated',
        r'å†…å®¹', r'ç»“æœ', r'è¾“å‡º'
    ]
    
    content_lower = content.lower()
    return any(re.search(pattern, content_lower) for pattern in important_patterns)

def contains_key_info(message: Dict) -> bool:
    """Check if message contains key information like file paths, URLs, etc."""
    content = message.get('content', '')
    if not content or not isinstance(content, str):
        return False
    
    key_patterns = [
        r'/[\w/.-]+',  # File paths
        r'http[s]?://\S+',  # URLs
        r'\w+\.(py|js|java|cpp|txt|md|json|yaml|yml)',  # File extensions
        r'^\s*def\s+\w+',  # Function definitions
        r'^\s*class\s+\w+',  # Class definitions
        r'^\s*import\s+\w+',  # Import statements
    ]
    
    for pattern in key_patterns:
        if re.search(pattern, content, re.MULTILINE):
            return True
    
    return False

def calculate_message_importance(message: Dict, index: int, total_messages: int) -> int:
    """
    Calculate importance score for a message (higher = more important).
    
    Scoring criteria:
    - System messages: 100
    - First user message: 90
    - Error messages: 85
    - Messages with key info: 80
    - Recent messages (last 3): 70 - 50 based on recency
    - Important tool results: 65
    - Tool messages: 40
    - User/Assistant messages: 30
    - Old messages: 10 - 0 based on age
    """
    role = message.get('role', '')
    
    # System messages are most important
    if role == 'system':
        return 100
    
    # First user message (initial request)
    if index == 1 and role == 'user':  # index 0 is usually system
        return 90
    
    # Error messages
    if is_error_message(message):
        return 85
    
    # Messages containing key information
    if contains_key_info(message):
        return 80
    
    # Recent messages (last 3 non-system messages get bonus)
    recency_bonus = max(0, 50 - (total_messages - index) * 10)
    if recency_bonus > 0:
        base_score = 30 if role in ['user', 'assistant'] else 40
        return base_score + recency_bonus
    
    # Important tool results
    if is_important_tool_result(message):
        return 65
    
    # Tool messages
    if role == 'tool':
        return 40
    
    # User/Assistant messages
    if role in ['user', 'assistant']:
        return 30
    
    # Default
    return 10

def find_tool_call_pairs(messages: List[Dict]) -> Tuple[Set[int], Dict[str, int]]:
    """
    Find indices of tool calls and their corresponding tool responses.
    Returns tuple of (set of indices to keep, mapping from tool_call_id to assistant message index).
    """
    keep_indices = set()
    
    # Map tool_call_id to assistant message index
    tool_call_to_assistant = {}
    
    # First pass: find assistant messages with tool calls
    for i, msg in enumerate(messages):
        if msg.get('role') == 'assistant' and msg.get('tool_calls'):
            keep_indices.add(i)
            # Track tool call IDs
            tool_calls = msg.get('tool_calls', [])
            # Ensure tool_calls is a list
            if not isinstance(tool_calls, list):
                tool_calls = []
            for tool_call in tool_calls:
                if isinstance(tool_call, dict) and 'id' in tool_call:
                    tool_call_to_assistant[tool_call['id']] = i
                elif hasattr(tool_call, 'id'):
                    tool_call_to_assistant[tool_call.id] = i
    
    # Second pass: find corresponding tool responses
    for i, msg in enumerate(messages):
        if msg.get('role') == 'tool':
            tool_call_id = msg.get('tool_call_id')
            if tool_call_id in tool_call_to_assistant:
                keep_indices.add(i)
    
    return keep_indices, tool_call_to_assistant

def enhance_summary(messages: List[Any], content: str) -> str:
    '''
    Use model-generated summary to optimize conversation history.
    
    :param messages: Conversation messages list (can be dicts or objects)
    :type messages: list
    :param content: Model-generated summary content
    :type content: str
    :return: JSON string containing optimized messages and statistics
    :rtype: str
    '''
    try:
        # Convert all messages to dict format for processing
        messages_dict = ensure_messages_dict(messages)
        
        if not content or not content.strip():
            return json.dumps({
                'error': 'Content cannot be empty',
                'original_message_count': len(messages_dict),
                'optimized_message_count': len(messages_dict),
                'optimization_percentage': 0.0,
                'summary_used': False
            }, ensure_ascii=False)
        

        # print("messages_dict:\n\n", json.dumps(messages_dict, ensure_ascii=False, indent=2))
        original_count = len(messages_dict)
       
        optimized_messages = []
        
        tool_find = False
        save_count = 15

        if original_count > 20:
            for i, msg in enumerate(messages_dict):
                if tool_find:
                    optimized_messages.append(msg)
                    tool_find = False
                    continue
                if i == original_count -save_count:
                    summary_message = {
                        'role': 'user',
                        'content': f'## Context Summary (Generated at {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")})\n\n{content}\n\n---\nNote: Previous conversation has been summarized above. Key messages are preserved below.'
                    }
                    optimized_messages.append(summary_message)
                if i > original_count - save_count:
                    if msg.get('role') == 'user' and not msg.get("content").startswith("## Context Optimization Needed") and not msg.get("content").startswith("## Context Summary"):
                        optimized_messages.append(msg)
                    if msg.get('role') == 'assistant':
                        tool_calls = msg.get('tool_calls', [])
                        # Ensure tool_calls is a list
                        if not isinstance(tool_calls, list):
                            tool_calls = []
                        if len(tool_calls) > 0:
                            optimized_messages.append(msg)
                            tool_find = True


                if msg.get('role') == 'system':
                    optimized_messages.append(msg)

                if msg.get('role') == 'user' and not msg.get("content").startswith("## Context Optimization Needed") and not msg.get("content").startswith("## Context Summary"):
                    optimized_messages.append(msg)
                
                if msg.get('role') == 'assistant':
                    tool_calls = msg.get('tool_calls', [])
                    # Ensure tool_calls is a list
                    if not isinstance(tool_calls, list):
                        tool_calls = []
                    if tool_calls and len(tool_calls) > 0:
                        for tool_call in tool_calls:
                            function = tool_call.get('function', {})

                            filter = {"get_project_summary"}
                            if function.get('name') in filter:
                                if hasattr(msg, "reasoning_content"):
                                    msg["reasoning_content"] = None
                                optimized_messages.append(msg)
                                tool_find = True
                
        # print("optimized_messages before tool pairing:\n\n", json.dumps(optimized_messages, ensure_ascii=False, indent=2))
        # must pair
        tool_calls_copy = []
        optimized_messages_copy = optimized_messages.copy()
        for i, msg in enumerate(optimized_messages_copy):
            if len(tool_calls_copy) >0:
                if msg.get('role') != 'tool':
                    print("Removing unpaired assistant tool_calls:", tool_calls)
                    optimized_messages.remove(msg)
                    tool_calls_copy=[]
                    continue

                tool_call_id = msg.get('tool_call_id')
                tmp = tool_calls_copy.copy()
                for i, tool_call in enumerate(tmp):
                    if tool_call.get("id") == tool_call_id:
                        tool_calls_copy.remove(tool_call)
                        continue
            
            if msg.get('role') == 'assistant':
                tool_calls = msg.get('tool_calls', [])
                # Ensure tool_calls is a list before copying
                if not isinstance(tool_calls, list):
                    tool_calls = []
                tool_calls_copy = tool_calls.copy()

        # Step 5: Calculate statistics
        optimized_count = len(optimized_messages)
        optimization_percentage = ((original_count - optimized_count) / original_count * 100) if original_count > 0 else 0
        
        # Calculate character reduction
        original_chars = sum(len(str(msg.get('content', ''))) for msg in messages_dict)
        optimized_chars = sum(len(str(msg.get('content', ''))) for msg in optimized_messages)
        char_reduction = ((original_chars - optimized_chars) / original_chars * 100) if original_chars > 0 else 0
        
        # Step 6: Update the original messages list in place
        # Clear and extend with optimized messages (all dicts)
        messages.clear()
        messages.extend(optimized_messages)
        
        print("messages after enhance_summary:\n\n", json.dumps(messages, ensure_ascii=False, indent=2))
        
        result = {
            'success': True,
            'original_message_count': original_count,
            'optimized_message_count': optimized_count,
            'optimization_percentage': round(optimization_percentage, 1),
            'char_reduction_percentage': round(char_reduction, 1),
            'summary_used': True,
            'summary_content': content[:200] + '...' if len(content) > 200 else content,
            'timestamp': datetime.datetime.now().isoformat(),
        }
        
        print(f"âœ… Context optimized: {original_count} â†’ {optimized_count} messages "
              f"({optimization_percentage:.1f}% reduction)")
        print(f"   Character reduction: {char_reduction:.1f}%")
        print(f"   Strategy: aggressive_retention_v2")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_msg = f'Error enhancing summary: {str(e)}'
        print(f"âŒ {error_msg}")
        import traceback
        traceback.print_exc()
        
        return json.dumps({
            'error': error_msg,
            'original_message_count': len(messages) if isinstance(messages, list) else 0,
            'optimized_message_count': 0,
            'optimization_percentage': 0.0,
            'summary_used': False
        }, ensure_ascii=False)

def update(messages_old: List[Any], messages_optimized: List[Any]) -> str:
    messages_old.clear()
    messages_old.extend(messages_optimized)
    return json.dumps({"len_optimized": len(messages_optimized)})

from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv("DEEPSEEK_API_KEY"), base_url="https://api.deepseek.com/")

def summary_by_ai(messages: List[Any]) -> str:

    tool_call_maps={
        "update": update
    }


    system_prompt = '''
You are a dialogue optimization expert, tasked with streamlining message history and enhancing structured summarization.

**Optimization Objectives:**
1. Limit the number of messages to less than 10 (`len(messages) < 10`)
2. Remove duplicate, redundant, and inconsequential dialogue content
3. Retain core decisions, key information, and important context
4. Add a new structured summary message

**Processing Rules:**
1. **Preserve System Messages**: Keep all necessary system role messages
2. **Core Dialogue Compression**: Merge consecutive same-role messages; remove greetings, acknowledgments, and other minor dialogues
3. **Key Information Extraction**: Retain parameter settings, goal definitions, decision nodes, and other critical information
4. **Timeline Preservation**: Maintain the chronological logic of the original dialogue

**New Summary Format (as the final message):**
{
  "role": "assistant",
  "content": "ã€Dialogue Summaryã€‘\nCurrent Status: [Brief description of current progress]\nCore Objective: [Clearly defined main goal]\nNext Action: [Specific tasks to be executed]"
}'''

    data = [{
        "role": "system",
        "content": system_prompt,
    },
    {
        "role": "user",
        # "content": f"'''{"this is a testing.  please invoke tool summary  to test"}''' \n\n "
        "content": f"'''{messages}''' \n\nold messages length is {len(messages)}.\ntarget messages length is less than 10.\n\n"
        "note:\n1. tool invoke must have result. "
    }]

    response = client.chat.completions.create(  
        model='deepseek-chat',
        messages=data,
        tools=[__ENHANCE_UPDATE_FUNCTION__],
        # extra_body={ "thinking": { "type": "enabled" } }
    )

    # Get response data
    message = response.choices[0].message
    if hasattr(message, "reasoning_content"):
        reasoning_content = message.reasoning_content
        print(f"ğŸ¤” Reasoning:\n{reasoning_content}")

    content = message.content
    tool_calls = message.tool_calls
    
    # Print to log
    if content:
        print(f"ğŸ’¬ Content: {content}")
    if tool_calls:
        print(f"ğŸ”§ Tool calls: {len(tool_calls)}")
        for i, tool in enumerate(tool_calls):
            print(f"  {i+1}. {tool.function.name}")
    else:
        print("ğŸ’¬ No tool calls.")
        return ""
    if tool_calls and len(tool_calls) > 0:
        print(f"message.model_dump(): {message.model_dump()}")
    # If there are no tool calls, we have a final answer
        
    # Execute tool calls with error handling
    for tool in tool_calls:
        try:
            tool_name = tool.function.name
            
            # Check if tool exists
            if tool_name not in tool_call_maps:
                error_msg = f"Error: Tool '{tool_name}' not found in tool call map"
                print(f"âŒ {error_msg}")
                continue
            
            # Parse arguments
            try:
                args = json.loads(tool.function.arguments)
            except json.JSONDecodeError as e:
                error_msg = f"Error: Invalid JSON arguments for tool '{tool_name}': {str(e)}"
                print(f"âŒ {error_msg}")
                
                continue
            
            # Execute tool
            tool_function = tool_call_maps[tool_name]
            print(f"ğŸ› ï¸  Executing: {tool_name}")
            print(f"   Arguments: {args}")
            if tool_name == "update":
                args["messages_old"] = messages
            try:
                tool_result = tool_function(**args)
                print(f"âœ… Tool result for {tool_name}: {tool_result[:500]}{'...' if len(tool_result) > 500 else ''}")
                    
            except Exception as e:
                error_msg = f"Error executing tool '{tool_name}': {str(e)}\n{traceback.format_exc()}"
                tool_result = error_msg
                print(f"âŒ {error_msg}")
            
            # Add tool result to messages

            
        except Exception as e:
            error_msg = f"Unexpected error processing tool call: {str(e)}\n{traceback.format_exc()}"
            print(f"âŒ {error_msg}")
    
    return json.dumps({
        "optimized_message_count": len(messages),
    })
# ============================================================================
# å·¥å…·è°ƒç”¨æ˜ å°„
# ============================================================================

# TOOL_CALL_MAP = {
#     "enhance_summary": enhance_summary
# }
TOOL_CALL_MAP = {
    "enhance_summary": enhance_summary
}

# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    print("AITools Summaryæ¨¡å—æµ‹è¯• (æ™ºèƒ½ä¿ç•™ç­–ç•¥ç‰ˆ)")
    print("=" * 60)
    
    # æµ‹è¯•æ•°æ® - æ¨¡æ‹Ÿä¸€ä¸ªå¤æ‚å¯¹è¯
    test_messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè½¯ä»¶ä¸“å®¶ï¼Œæ‹¥æœ‰æ“ä½œç³»ç»Ÿã€æ–‡ä»¶ã€ç½‘ç»œã€Gitç­‰å·¥å…·ã€‚è¯·è°¨æ…ä½¿ç”¨å·¥å…·ï¼Œå¿…è¦æ—¶å‘ç”¨æˆ·ç¡®è®¤ã€‚"},
        {"role": "user", "content": "è¯·å¸®æˆ‘åˆ›å»ºä¸€ä¸ªPythoné¡¹ç›®ï¼Œç”¨äºå¤„ç†æ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€‚"},
        {"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘å°†å¸®æ‚¨åˆ›å»ºPythoné¡¹ç›®ã€‚è¯·å‘Šè¯‰æˆ‘é¡¹ç›®åç§°å’Œéœ€è¦çš„åŠŸèƒ½ã€‚"},
        {"role": "user", "content": "é¡¹ç›®åä¸ºDataAnalyzerï¼Œéœ€è¦pandas, matplotlib, seabornåº“ã€‚"},
        {"role": "assistant", "content": "æ­£åœ¨åˆ›å»ºé¡¹ç›®DataAnalyzer..."},
        {"role": "tool", "tool_call_id": "call_123", "content": "é¡¹ç›®ç›®å½•åˆ›å»ºæˆåŠŸ"},
        {"role": "assistant", "content": "æ­£åœ¨å®‰è£…ä¾èµ–åº“..."},
        {"role": "tool", "tool_call_id": "call_456", "content": "Error: ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œæ— æ³•å®‰è£…pandas"},
        {"role": "assistant", "content": "å®‰è£…é‡åˆ°é”™è¯¯ï¼Œæ­£åœ¨å°è¯•å…¶ä»–æ–¹æ³•..."},
        {"role": "tool", "tool_call_id": "call_789", "content": "å·²ä½¿ç”¨æœ¬åœ°ç¼“å­˜å®‰è£…pandas, matplotlib, seaborn"},
        {"role": "assistant", "content": "ä¾èµ–å®‰è£…æˆåŠŸã€‚æ­£åœ¨åˆ›å»ºç¤ºä¾‹ä»£ç ..."},
        {"role": "tool", "tool_call_id": "call_999", "content": "ç¤ºä¾‹ä»£ç å·²åˆ›å»º: data_analyzer.py"},
        {"role": "assistant", "content": "é¡¹ç›®åˆ›å»ºå®Œæˆï¼åŒ…å«æ•°æ®åˆ†æå’Œå¯è§†åŒ–ç¤ºä¾‹ä»£ç ã€‚"},
        {"role": "user", "content": "è°¢è°¢ï¼ç°åœ¨è¯·å¸®æˆ‘è¿è¡Œæµ‹è¯•ï¼Œç¡®ä¿ä¸€åˆ‡æ­£å¸¸ã€‚"},
        {"role": "assistant", "content": "æ­£åœ¨è¿è¡Œæµ‹è¯•..."},
        {"role": "tool", "tool_call_id": "call_101", "content": "æµ‹è¯•é€šè¿‡ï¼šæ‰€æœ‰åŠŸèƒ½æ­£å¸¸"},
    ]
    
    # æµ‹è¯•æ€»ç»“å¢å¼º
    print("æµ‹è¯•enhance_summaryå‡½æ•°:")
    test_summary = "ç”¨æˆ·è¯·æ±‚åˆ›å»ºPythonæ•°æ®åˆ†æé¡¹ç›®DataAnalyzerã€‚åŠ©æ‰‹åˆ›å»ºäº†é¡¹ç›®ç›®å½•ï¼Œå®‰è£…ä¾èµ–æ—¶é‡åˆ°ç½‘ç»œé”™è¯¯ä½†é€šè¿‡æœ¬åœ°ç¼“å­˜è§£å†³ï¼Œåˆ›å»ºäº†ç¤ºä¾‹ä»£ç ï¼Œæœ€åè¿è¡Œæµ‹è¯•é€šè¿‡ã€‚"
    print(f"åŸå§‹æ¶ˆæ¯æ•°: {len(test_messages)}")
    print(f"æ€»ç»“å†…å®¹: {test_summary[:100]}...")
    print()
    
    # åˆ›å»ºå‰¯æœ¬ç”¨äºæµ‹è¯•
    test_messages_copy = test_messages.copy()
    
    result = enhance_summary(test_messages_copy, test_summary)
    print("\nä¼˜åŒ–ç»“æœ:")
    print(result[:500] + "..." if len(result) > 500 else result)
    print()
    
    # è§£æç»“æœ
    try:
        result_dict = json.loads(result)
        print(f"åŸå§‹æ¶ˆæ¯æ•°: {result_dict.get('original_message_count')}")
        print(f"ä¼˜åŒ–åæ¶ˆæ¯æ•°: {result_dict.get('optimized_message_count')}")
        print(f"ä¼˜åŒ–æ¯”ä¾‹: {result_dict.get('optimization_percentage', 0):.1f}%")
        print(f"å­—ç¬¦å‡å°‘æ¯”ä¾‹: {result_dict.get('char_reduction_percentage', 0):.1f}%")
        print(f"ä¿ç•™çš„æ¶ˆæ¯ç±»å‹: {result_dict.get('kept_message_types')}")
        print(f"å¿…é¡»ä¿ç•™çš„æ¶ˆæ¯æ•°: {result_dict.get('must_keep_count')}")
        print(f"æŒ‰é‡è¦æ€§é€‰æ‹©çš„æ¶ˆæ¯æ•°: {result_dict.get('selected_by_importance')}")
        print(f"ä¼˜åŒ–ç­–ç•¥: {result_dict.get('optimization_strategy')}")
        print()
        
        # æ˜¾ç¤ºä¼˜åŒ–åçš„æ¶ˆæ¯
        print("ä¼˜åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨:")
        for i, msg in enumerate(test_messages_copy):
            role = msg.get('role', 'unknown')
            content_preview = str(msg.get('content', ''))[:80].replace('\n', ' ')
            print(f"  {i}. [{role}] {content_preview}{'...' if len(str(msg.get('content', ''))) > 80 else ''}")
            
    except Exception as e:
        print(f"è§£æç»“æœæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nSummaryæ¨¡å—æµ‹è¯•å®Œæˆ!")
